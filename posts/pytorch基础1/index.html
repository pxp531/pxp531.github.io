<!doctype html>



































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="zh-cn"
  dir="ltr"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Pytorch基础1 - pxxp</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="| 为什么学它？之前只会TensorFlow，torch现在用的更多，有时间了解一下，顺便记录一下。
基础1-Tensor 定义 在pytorch里面，模型的输入输出以及参数都用tensor表示。 tensor和numpy里面的ndarray很像，且它们底层是共享的，需要注意。numpy也能写神经网络，tensor的优势是什么呢？可以用GPU等异构计算加速。 tensor可以自动求导，这个在深度学习里面很关键，深度学习最终就是要找到一堆合适的参数去最小化误差，找参数的这个过程用的是梯度下降，就需要求参数的导数，tensor可以自动求导就很方便。 使用 导包 import torch 初始化 初始化有很多种方式，比如说数据直接转换成tensor、从numpy的数组转换成tensor等
数据直接转换成tensor：用torch.tensor()包一层
data = [[1,2], [3,4]] x_data = torch.tensor(data) numpy数组转成tensor：torch.from_numpy()包一层
np_arr = np.array(data) x_np = torch.from_numpy(np_arr) 从其他tensor转换：形状不变，值变化
torch.ones_like() torch.rand_like() 随机数or常量：随机初始化一个tensor
torch.rand() -&gt; 里面传一个元组 torch.ones() torch.zeros() tensor属性 shape
datatype
device：在cpu or gpu上
创建的时候默认是在CPU上，需要显示的移动到GPU上。tensor.to(&ldquo;cuda&rdquo;) tensor操作 索引 &amp; 切片
tensor = torch.ones(4, 4) print(tensor[0]) print(tensor[:,0]) print(tensor[...,-1]) tensor[:,1] = 0 合并
神经网络里面会有网络之间的合并，就是tensor的合并，比如多头注意力机制最后每个头会进行一个合并，使用的是cat 参数dim是指定按照行 or 列来进行合并 算术运算
最多的应该是element-wise以及矩阵乘法了
t1.matmul(t2) -&gt; 矩阵乘法 t1.mul(t2) -&gt; element-wise乘法 聚合运算" />
  <meta name="author" content="pxxp" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://pxp531.github.io/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="https://pxp531.github.io/theme.png" />

  
  
  
  
  

  
  
  <link rel="preload" as="image" href="https://pxp531.github.io/github.svg" />
  
  <link rel="preload" as="image" href="https://pxp531.github.io/rss.svg" />
  
  

  
  
  <script
    defer
    src="https://pxp531.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link
    rel="icon"
    href="https://pxp531.github.io/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="https://pxp531.github.io/apple-touch-icon.png"
  />

  
  <meta name="generator" content="Hugo 0.133.0">

  
  
  
  
  


  
  
  <meta itemprop="name" content="Pytorch基础1">
  <meta itemprop="description" content="| 为什么学它？之前只会TensorFlow，torch现在用的更多，有时间了解一下，顺便记录一下。
基础1-Tensor 定义 在pytorch里面，模型的输入输出以及参数都用tensor表示。 tensor和numpy里面的ndarray很像，且它们底层是共享的，需要注意。numpy也能写神经网络，tensor的优势是什么呢？可以用GPU等异构计算加速。 tensor可以自动求导，这个在深度学习里面很关键，深度学习最终就是要找到一堆合适的参数去最小化误差，找参数的这个过程用的是梯度下降，就需要求参数的导数，tensor可以自动求导就很方便。 使用 导包 import torch 初始化 初始化有很多种方式，比如说数据直接转换成tensor、从numpy的数组转换成tensor等
数据直接转换成tensor：用torch.tensor()包一层
data = [[1,2], [3,4]] x_data = torch.tensor(data) numpy数组转成tensor：torch.from_numpy()包一层
np_arr = np.array(data) x_np = torch.from_numpy(np_arr) 从其他tensor转换：形状不变，值变化
torch.ones_like() torch.rand_like() 随机数or常量：随机初始化一个tensor
torch.rand() -&gt; 里面传一个元组 torch.ones() torch.zeros() tensor属性 shape
datatype
device：在cpu or gpu上
创建的时候默认是在CPU上，需要显示的移动到GPU上。tensor.to(“cuda”) tensor操作 索引 &amp; 切片
tensor = torch.ones(4, 4) print(tensor[0]) print(tensor[:,0]) print(tensor[...,-1]) tensor[:,1] = 0 合并
神经网络里面会有网络之间的合并，就是tensor的合并，比如多头注意力机制最后每个头会进行一个合并，使用的是cat 参数dim是指定按照行 or 列来进行合并 算术运算
最多的应该是element-wise以及矩阵乘法了
t1.matmul(t2) -&gt; 矩阵乘法 t1.mul(t2) -&gt; element-wise乘法 聚合运算">
  <meta itemprop="datePublished" content="2024-09-03T21:50:24+08:00">
  <meta itemprop="dateModified" content="2024-09-03T21:50:24+08:00">
  <meta itemprop="wordCount" content="89">
  
  <meta property="og:url" content="https://pxp531.github.io/posts/pytorch%E5%9F%BA%E7%A1%801/">
  <meta property="og:site_name" content="pxxp">
  <meta property="og:title" content="Pytorch基础1">
  <meta property="og:description" content="| 为什么学它？之前只会TensorFlow，torch现在用的更多，有时间了解一下，顺便记录一下。
基础1-Tensor 定义 在pytorch里面，模型的输入输出以及参数都用tensor表示。 tensor和numpy里面的ndarray很像，且它们底层是共享的，需要注意。numpy也能写神经网络，tensor的优势是什么呢？可以用GPU等异构计算加速。 tensor可以自动求导，这个在深度学习里面很关键，深度学习最终就是要找到一堆合适的参数去最小化误差，找参数的这个过程用的是梯度下降，就需要求参数的导数，tensor可以自动求导就很方便。 使用 导包 import torch 初始化 初始化有很多种方式，比如说数据直接转换成tensor、从numpy的数组转换成tensor等
数据直接转换成tensor：用torch.tensor()包一层
data = [[1,2], [3,4]] x_data = torch.tensor(data) numpy数组转成tensor：torch.from_numpy()包一层
np_arr = np.array(data) x_np = torch.from_numpy(np_arr) 从其他tensor转换：形状不变，值变化
torch.ones_like() torch.rand_like() 随机数or常量：随机初始化一个tensor
torch.rand() -&gt; 里面传一个元组 torch.ones() torch.zeros() tensor属性 shape
datatype
device：在cpu or gpu上
创建的时候默认是在CPU上，需要显示的移动到GPU上。tensor.to(“cuda”) tensor操作 索引 &amp; 切片
tensor = torch.ones(4, 4) print(tensor[0]) print(tensor[:,0]) print(tensor[...,-1]) tensor[:,1] = 0 合并
神经网络里面会有网络之间的合并，就是tensor的合并，比如多头注意力机制最后每个头会进行一个合并，使用的是cat 参数dim是指定按照行 or 列来进行合并 算术运算
最多的应该是element-wise以及矩阵乘法了
t1.matmul(t2) -&gt; 矩阵乘法 t1.mul(t2) -&gt; element-wise乘法 聚合运算">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-03T21:50:24+08:00">
    <meta property="article:modified_time" content="2024-09-03T21:50:24+08:00">

  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Pytorch基础1">
  <meta name="twitter:description" content="| 为什么学它？之前只会TensorFlow，torch现在用的更多，有时间了解一下，顺便记录一下。
基础1-Tensor 定义 在pytorch里面，模型的输入输出以及参数都用tensor表示。 tensor和numpy里面的ndarray很像，且它们底层是共享的，需要注意。numpy也能写神经网络，tensor的优势是什么呢？可以用GPU等异构计算加速。 tensor可以自动求导，这个在深度学习里面很关键，深度学习最终就是要找到一堆合适的参数去最小化误差，找参数的这个过程用的是梯度下降，就需要求参数的导数，tensor可以自动求导就很方便。 使用 导包 import torch 初始化 初始化有很多种方式，比如说数据直接转换成tensor、从numpy的数组转换成tensor等
数据直接转换成tensor：用torch.tensor()包一层
data = [[1,2], [3,4]] x_data = torch.tensor(data) numpy数组转成tensor：torch.from_numpy()包一层
np_arr = np.array(data) x_np = torch.from_numpy(np_arr) 从其他tensor转换：形状不变，值变化
torch.ones_like() torch.rand_like() 随机数or常量：随机初始化一个tensor
torch.rand() -&gt; 里面传一个元组 torch.ones() torch.zeros() tensor属性 shape
datatype
device：在cpu or gpu上
创建的时候默认是在CPU上，需要显示的移动到GPU上。tensor.to(“cuda”) tensor操作 索引 &amp; 切片
tensor = torch.ones(4, 4) print(tensor[0]) print(tensor[:,0]) print(tensor[...,-1]) tensor[:,1] = 0 合并
神经网络里面会有网络之间的合并，就是tensor的合并，比如多头注意力机制最后每个头会进行一个合并，使用的是cat 参数dim是指定按照行 or 列来进行合并 算术运算
最多的应该是element-wise以及矩阵乘法了
t1.matmul(t2) -&gt; 矩阵乘法 t1.mul(t2) -&gt; element-wise乘法 聚合运算">

  
  

  
  <link rel="canonical" href="https://pxp531.github.io/posts/pytorch%E5%9F%BA%E7%A1%801/" />
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center">
  <div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center">
    <a class="-translate-y-[1px] text-2xl font-medium" href="https://pxp531.github.io/"
      >pxxp</a
    >
    <div
      class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"
    >
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/pxp531"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href="https://pxp531.github.io/index.xml"
        target="_blank"
        rel="alternate"
      >
        rss
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
  <header class="mb-14">
    <h1 class="!my-0 pb-2.5">Pytorch基础1</h1>

    
    <div class="text-xs antialiased opacity-60">
      
      <time>Sep 3, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>| 为什么学它？之前只会TensorFlow，torch现在用的更多，有时间了解一下，顺便记录一下。</p>
<h2 id="基础1-tensor">基础1-Tensor</h2>
<h3 id="定义">定义</h3>
<ul>
<li>在pytorch里面，模型的输入输出以及参数都用tensor表示。</li>
<li>tensor和numpy里面的ndarray很像，且它们底层是共享的，需要注意。numpy也能写神经网络，tensor的优势是什么呢？可以用GPU等异构计算加速。</li>
<li>tensor可以自动求导，这个在深度学习里面很关键，深度学习最终就是要找到一堆合适的参数去最小化误差，找参数的这个过程用的是梯度下降，就需要求参数的导数，tensor可以自动求导就很方便。</li>
</ul>
<h3 id="使用">使用</h3>
<h4 id="导包">导包</h4>
<ul>
<li>import torch</li>
</ul>
<h4 id="初始化">初始化</h4>
<ul>
<li>
<p>初始化有很多种方式，比如说数据直接转换成tensor、从numpy的数组转换成tensor等</p>
<ul>
<li>
<p><strong>数据直接转换成tensor</strong>：用torch.tensor()包一层</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>]]
</span></span><span style="display:flex;"><span>x_data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(data)
</span></span></code></pre></div></li>
<li>
<p><strong>numpy数组转成tensor</strong>：torch.from_numpy()包一层</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np_arr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(data)
</span></span><span style="display:flex;"><span>x_np <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>from_numpy(np_arr)
</span></span></code></pre></div></li>
<li>
<p><strong>从其他tensor转换</strong>：形状不变，值变化</p>
<ul>
<li>torch.ones_like()</li>
<li>torch.rand_like()</li>
</ul>
</li>
<li>
<p><strong>随机数or常量</strong>：随机初始化一个tensor</p>
<ul>
<li>torch.rand() -&gt; 里面传一个元组</li>
<li>torch.ones()</li>
<li>torch.zeros()</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="tensor属性">tensor属性</h4>
<ul>
<li>
<p>shape</p>
</li>
<li>
<p>datatype</p>
</li>
<li>
<p>device：在cpu or gpu上</p>
<ul>
<li>创建的时候默认是在CPU上，需要显示的移动到GPU上。tensor.to(&ldquo;cuda&rdquo;)</li>
</ul>
</li>
</ul>
<h4 id="tensor操作">tensor操作</h4>
<ul>
<li>
<p>索引 &amp; 切片</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>  print(tensor[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>  print(tensor[:,<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>  print(tensor[<span style="color:#f92672">...</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>  tensor[:,<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>合并</p>
<ul>
<li>神经网络里面会有网络之间的合并，就是tensor的合并，比如多头注意力机制最后每个头会进行一个合并，使用的是cat</li>
<li>参数dim是指定按照行 or 列来进行合并</li>
</ul>
</li>
<li>
<p>算术运算</p>
<ul>
<li>
<p>最多的应该是element-wise以及矩阵乘法了</p>
<ul>
<li>t1.matmul(t2) -&gt; 矩阵乘法</li>
<li>t1.mul(t2) -&gt; element-wise乘法</li>
</ul>
</li>
<li>
<p>聚合运算</p>
<ul>
<li>agg = tensor.sum() -&gt; 元素求和，但是还是tensor</li>
<li>agg_py = agg.item() -&gt; 将单个tensor转换成Python</li>
</ul>
</li>
<li>
<p>原地修改</p>
<ul>
<li>语法糖：在运算符后面加一个下划线，比如add_()</li>
<li>原地操作会删除掉之前的，如果确认后面用不到了就可以，或者保存覆盖的代价比重算的代价更小就也可以用这个。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="numpy底层共享">numpy底层共享</h4>
<ul>
<li>这里要比较注意，从tensor转numpy，或者numpy转tensor底层默认是共享的，是一个深拷贝的操作。</li>
<li>numpy转tensor，使用的是torch.from_numpy，这里是一个共享的，如果后面修改了这个tensor，原来的numpy也会变。</li>
<li>tensor转numpy：t.numpy()</li>
</ul>
</section>

  
  

  
  
  
  
  <nav
    class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
  >
    
    <a class="ltr:pr-3 rtl:pl-3" href="https://pxp531.github.io/posts/perf%E5%8E%9F%E7%90%8602/"
      ><span class="ltr:mr-1.5 rtl:ml-1.5">←</span><span>Perf原理02</span></a
    >
    
    
    <a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href="https://pxp531.github.io/posts/perf%E5%8E%9F%E7%90%8601/"
      ><span>Perf原理01</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
    >
    
  </nav>
  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
  <div class="mr-auto">
  
    &copy; 2024
    <a class="link" href="https://pxp531.github.io/">pxxp</a>
  
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>

  </body>
</html>
